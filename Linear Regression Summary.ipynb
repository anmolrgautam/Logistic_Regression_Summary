{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "084a0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "1a0fdfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[1,2,2],[2,5,4], [3,1,2], [6,2,7],[10,4,1],[3,1,2]]\n",
    "X = pd.DataFrame(X,columns=['A','B','C'])\n",
    "y = [1,2,1,3,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "91a122a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcLogLikelihood(guess, true, n):\n",
    "    import math\n",
    "    error = true-guess\n",
    "    sigma = np.std(error)\n",
    "    f = ((1.0/(2.0*math.pi*sigma*sigma))**(n/2))* \\\n",
    "        np.exp(-1*((np.dot(error.T,error))/(2*sigma*sigma)))\n",
    "    return np.log(f)\n",
    "\n",
    "def calcDurbinWatson(guess, true):\n",
    "    e = true-guess\n",
    "    su = 0\n",
    "    su1 = 0\n",
    "    for i in range(len(e)):\n",
    "        if i+1<len(e):\n",
    "            su += (e[i+1] - e[i])**2\n",
    "        su1 += (e[i])**2\n",
    "    return su/su1\n",
    "\n",
    "def calConditionNumber(x):\n",
    "    # Getting the singular values from SVD\n",
    "    _, sing_as, _ = np.linalg.svd(x)\n",
    "\n",
    "    cond_no = sing_as.max()/sing_as.min()\n",
    "    return cond_no\n",
    "\n",
    "def LinearRegressionSummary(lm, X, y):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy import stats\n",
    "    \n",
    "    params = np.append(lm.intercept_,lm.coef_)\n",
    "    pred = lm.predict(X)\n",
    "    \n",
    "    newX = pd.DataFrame({\"Constant\":np.ones(len(X))}).join(pd.DataFrame(X))\n",
    "    \n",
    "    n = len(X)\n",
    "    k = X.shape[1]\n",
    "    df_res = n - k - 1\n",
    "    df_model = k\n",
    "    score = lm.score(X,y)\n",
    "    \n",
    "    MSE = (sum((y-pred)**2))/(df_res)\n",
    "    var_b = MSE*(np.linalg.inv(np.dot(newX.T,newX)).diagonal())\n",
    "    sd_b = np.sqrt(var_b)\n",
    "    ts_b = params/ sd_b\n",
    "    \n",
    "    p_values = [2*(1-stats.t.cdf(np.abs(i),df_res)) for i in ts_b]\n",
    "    lower = [params[i] - stats.t.ppf(q = 0.975, df = df_res) * sd_b[i] for i in range(k+1)]\n",
    "    upper = [params[i] + stats.t.ppf(q = 0.975, df = df_res) * sd_b[i] for i in range(k+1)]\n",
    "    \n",
    "    sd_b = np.round(sd_b,3)\n",
    "    ts_b = np.round(ts_b,3)\n",
    "    params = np.round(params,4)\n",
    "    p_values = np.round(p_values,3)\n",
    "    lower = np.round(lower,3)\n",
    "    upper = np.round(upper,3)\n",
    "    \n",
    "    print('Logistic Regression Summary'.center(78))\n",
    "    print('='*78)\n",
    "    print('Model: Logistic Regression')\n",
    "    print('No. of Observations:',n)\n",
    "    print('Df Resifuals:', df_res)\n",
    "    print('Df Model:',df_model)\n",
    "    print('R-squared:', np.round(score,3))\n",
    "    print('Adj. R-squared:', np.round(1 - (1-score)*(n-1)/(df_res),3))\n",
    "    print('F-statistic:', np.round((score/(1-score))*(df_res/k),3))\n",
    "    print('Prob (F-statistic):', np.round(1-stats.f.cdf((score/(1-score))*(df_res/k), k, df_res),6))\n",
    "    print('Log-Likelihood:', np.round(calcLogLikelihood(pred, y, n),3))\n",
    "    print('AIC:', np.round(-2*(calcLogLikelihood(pred, y, n)) + 2*(k+1),3))\n",
    "    print('BIC:', np.round(-2*(calcLogLikelihood(pred, y, n)) + np.log(n) * (k+1),3))\n",
    "    if (len(y)) >=8:\n",
    "        print('Omnibus:', np.round(stats.normaltest(y-pred)[0],3))\n",
    "        print('Prob(Omnibus):', np.round(stats.normaltest(y-pred)[1],3))\n",
    "    else:\n",
    "        print('Omnibus:', np.nan)\n",
    "        print('Prob(Omnibus):', np.nan)\n",
    "    print('Skewness:', np.round(stats.skew(y-pred),3))\n",
    "    print('Kurtosis:', np.round(stats.kurtosis(y-pred,fisher=False),3))\n",
    "    print('Durbin-Watson:', np.round(calcDurbinWatson(pred, y),3))\n",
    "    print('Jarque-Bera (JB):',np.round(stats.jarque_bera(y-pred)[0],3))\n",
    "    print('Prob(JB):',np.round(stats.jarque_bera(y-pred)[1],3))\n",
    "    print('Condition Number:',np.round(calConditionNumber(newX),3))\n",
    "    print('='*78)\n",
    "    #print('\\n')\n",
    "\n",
    "    myDF3 = pd.DataFrame()\n",
    "    myDF3['Features'],myDF3[\"Coef\"],myDF3[\"std err\"],myDF3[\"t values\"],myDF3[\"P>|t|\"], myDF3['[0.025'], myDF3['0.975]'] = [newX.columns,params,sd_b,ts_b,p_values, lower,upper]\n",
    "    print(myDF3.to_string().replace('\\n0', '\\n'+'-'*78+'\\n0'))\n",
    "    print('='*78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "a3568206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Logistic Regression Summary                          \n",
      "==============================================================================\n",
      "Model: Logistic Regression\n",
      "No. of Observations: 6\n",
      "Df Resifuals: 2\n",
      "Df Model: 3\n",
      "R-squared: 1.0\n",
      "Adj. R-squared: 1.0\n",
      "F-statistic: 7144.875\n",
      "Prob (F-statistic): 0.00014\n",
      "Log-Likelihood: 20.942\n",
      "AIC: -33.885\n",
      "BIC: -34.718\n",
      "Omnibus: nan\n",
      "Prob(Omnibus): nan\n",
      "Skewness: -0.852\n",
      "Kurtosis: 2.456\n",
      "Durbin-Watson: 1.684\n",
      "Jarque-Bera (JB): 0.8\n",
      "Prob(JB): 0.67\n",
      "Condition Number: 16.917\n",
      "==============================================================================\n",
      "   Features    Coef  std err  t values  P>|t|  [0.025  0.975]\n",
      "------------------------------------------------------------------------------\n",
      "0  Constant  0.1036    0.014     7.613  0.017   0.045   0.162\n",
      "1         A  0.0246    0.002    13.697  0.005   0.017   0.032\n",
      "2         B  0.0698    0.004    19.251  0.003   0.054   0.085\n",
      "3         C  0.3731    0.003   142.689  0.000   0.362   0.384\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "lm = LinearRegression()#positive=True)\n",
    "lm.fit(X,y)\n",
    "\n",
    "LinearRegressionSummary(lm,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "12023dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       1.000\n",
      "Model:                            OLS   Adj. R-squared:                  1.000\n",
      "Method:                 Least Squares   F-statistic:                     7145.\n",
      "Date:                Thu, 26 May 2022   Prob (F-statistic):           0.000140\n",
      "Time:                        17:20:13   Log-Likelihood:                 20.942\n",
      "No. Observations:                   6   AIC:                            -33.88\n",
      "Df Residuals:                       2   BIC:                            -34.72\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          0.1036      0.014      7.613      0.017       0.045       0.162\n",
      "A              0.0246      0.002     13.697      0.005       0.017       0.032\n",
      "B              0.0698      0.004     19.251      0.003       0.054       0.085\n",
      "C              0.3731      0.003    142.689      0.000       0.362       0.384\n",
      "==============================================================================\n",
      "Omnibus:                          nan   Durbin-Watson:                   1.684\n",
      "Prob(Omnibus):                    nan   Jarque-Bera (JB):                0.800\n",
      "Skew:                          -0.852   Prob(JB):                        0.670\n",
      "Kurtosis:                       2.456   Cond. No.                         16.9\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anmol.gautam\\Miniconda3\\envs\\ta-tigerml\\lib\\site-packages\\statsmodels\\stats\\stattools.py:74: ValueWarning: omni_normtest is not valid with less than 8 observations; 6 samples were given.\n",
      "  warn(\"omni_normtest is not valid with less than 8 observations; %i \"\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "X2 = sm.add_constant(X)\n",
    "est = sm.OLS(y, X2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
